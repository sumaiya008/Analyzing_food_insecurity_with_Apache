# -*- coding: utf-8 -*-
"""Analyzing_food_insecurity_with_Apache

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KE5lHycY_b8-PrIfi1cQUer_4nfGH0y0

# Apache Spark - Analyzing Food Insecurity in NYC using KeyFoods Price Catalogs

In this notebook, we study the food insecurity problem by looking at the listed prices of various food items across neighborhoods in NYC. Our hypothesis is that *people living in areas with higher food insecurity problems would pay more for the same items compared to those in more secured areas*. For the scope of work, we will only assess food products from KeyFoods supermarkets, one of the top 4 Supermarket Leaders in Metro New York (according [Food Trade News 2021 report](https://www.foodtradenews.com/2021/06/29/food-trade-news-2021-market-study-issue/)). In particular, we will use the following datasets:

### **`keyfood_products.csv`**

This CSV file contains the price information about 2 million food items listed on KeyFoods stores in NYC.

|store|department|upc|product|size|price|
|--|--|--|--|--|--|
102|bakery|102-28556000000|Store Prepared - Challah Egg|1 EA|\$4.99 each|
102|bakery|102-28781600000|Store Prepared - fw Cheesecake Plain 7 Inch|1 EA|\$27.99 each|
|...|...|...|...|...|...|

The details of the columns are as follows:

|Column|Description|
|--|--|
|**store** | The unique id of each store |
|**department**| The department (or aisle) that the food item belongs to. Possible values are:<br />`'bakery'`,`'beverages'`,`'breakfast'`,`'deli'`,`'frozen'`,`'international'`,<br/>`'meatandseafood'`,`'pantry'`,`'produce'`,`'refrigerated'`, and `'snacks'`|
|**upc**|The unique id for each item in a food store. It is often in the format of `SID-XXXXXXXXXX`,<br/> where `SID` is a store id if it's specific to a store, `UPC` if it's a general product, or `'KEY'` <br/> if it's a KeyFoodsproduct. If an item doesn't have any UPC code, this field will be `N/A`.|
|**product**|This is the listed name of the product|
|**size**|The unit that the product is being sold in|
|**price**|The price text of the product as shown on their websites. This is not a number but have<br/>been verified to start with the price mark`$XX.XX`. Note that for items without price<br/>information, this field could be listed as `Price not Available`|



This is the *big data* part of the homework, where we need to use Apache Spark to process it.

### **`keyfood_nyc_stores.json`**

This JSON file contains information for all KeyFoods stores in NYC. There are a lot of details about each store, however, for this homework, we are only interested in the following fields:

|Field|Description|
|--|--|
|**name** | This is the unique id of each store, which could be crosswalk with the **store** field above |
|**communityDistrict**|The community district code that the store belongs to. It's simply a larger geographical<br/> unit comparing to a zip code. More information can be found [here](https://communityprofiles.planning.nyc.gov/).|
|**foodInsecurity**|A food insecurity score computed for the community district that the stores belong to.<br/> This value has the range of 0 to 1 with 0 being without any food insecurity rish, and 1 <br/> has the most food insecure risk.|

### **`keyfood_sample_items.csv`**

This data contains the list of 22 food items that we would like to study initially to assess our hypothesis. For each item, we have the UPC code (which needs to be generalized across store) and the item name. Here is the list:

|UPC code|Item Name|
|--|--|
|SID-20308200000|Broccoli Crowns|
|KEY-000000004094|Fresh Produce - Carrot Bunch|
|KEY-000000004062|Fresh Produce - Cucumbers|
|SID-00000004072|Fresh Produce - Potatoes Russet|
|SID-00000004131|Fresh Produce - Apples Fuji Large|
|KEY-00000004013|Produce - Orange Navel 113|
|UPC-048500001004|Tropicana - Juice Orange Pure Prem Orig|
|UPC-017400108700|Carolina - Whole Grain Brown Rice|
|UPC-016000487697|General Mills - Cherrios Multi Grain Cereal|
|UPC-073296027686|Urban Meadow - 100 Whole Wheat Bread|
|UPC-048000013651|Chicken of the Sea - Solid Wht Albacore Tuna in Oil|
|SID-20115200000|Beef - Beef Semi Bnls Chuck Stk|
|SID-28080600000|Perdue - Split Chicken Breast Fam Pack|
|UPC-073296057461|Urban Meadow - Plain Low Fat Yogurt|
|UPC-041757021443|Laughing Cow - White Cheddar Wedges|
|UPC-073296069280|Urban Meadow - Large White Eggs|
|UPC-088365000347|Cream O Land - Gallon 2% Milk|
|UPC-072940744016|Redpack - Tomato Crushed|
|UPC-051500255162|Jif - Creamy Peanut Butter|
|UPC-073296025903|Urban Meadow - Canola Oil|
|UPC-041331124461|Goya - Beans Cannelini Can|
|UPC-014500001894|Birds Eye - Spinach Leaf|

where `SID` should be replaced with the store id.

### Notes

* There are 3 tasks below. You can use Google Colab for the first 2 tasks. Task 3 requires the use of NYU HPC.

* Our big data set (`keyfood_products.csv`) is assumed to be on HDFS, and must be accessed using Spark (either as RDD or DataFrame).

* You are not allowed to collect the raw data to the notebook and process them without using Spark. However, it is okay to collect intermediate data for processing. Just try to collect as little as possible.

## Environment Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# gdown --quiet 1O1U_t-cpmValVK2mjdTzcFxIbGw05vOw
# gdown --quiet 1YUBKrtNV3QUz1RutMnMbJdQj7rv-Lkd5
# gdown --quiet 1f79oETtvN3NQLYPnVGhurE1UBDP4IQP-
# pip install pyspark

# Commented out IPython magic to ensure Python compatibility.
import csv
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import IPython
# %matplotlib inline
IPython.display.set_matplotlib_formats('svg')
pd.plotting.register_matplotlib_converters()
sns.set_style("whitegrid")

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
sc = pyspark.SparkContext.getOrCreate()
spark = SparkSession(sc)
spark

"""## Task 1 - Visualizing Distributions of Listed Food Prices

In the first task, we would like to see how the listed prices for food items vary across stores. For each item in the sample list provided in `keyfood_sample_items.csv`, we can simply overlay a [strip plot](https://seaborn.pydata.org/generated/seaborn.stripplot.html) with a [violin plot](https://seaborn.pydata.org/generated/seaborn.violinplot.html). In addition, to better correlate the price distribution with the food insecurity risk, we will color the markers by the percentage of food insecurity (derived from the `foodInsecurity` field in `keyfood_nyc_stores.json`). An expected visualization is provided below:
"""

#@title

"""To produce the plot, we need to following data, where each row represents a listing of the sample food item from a store.

| Item Name	| Price ($) | % Food Insecurity |
|--|--|--|
|Urban Meadow - 100 Whole Wheat Bread | 2.29 | 11 |
|General Mills - Cherrios Multi Grain Cereal | 6.79 | 11 |
|Birds Eye - Spinach Leaf | 2.29 | 11 |
|Beef - Beef Semi Bnls Chuck Stk | 7.99 | 11 |
|Chicken of the Sea - Solid Wht Albacore Tuna in Oil | 2.49 | 11 |
| ... | ... | ... |

Your task is to compute the above table from the input data.

### INPUT
**You must read `keyfood_products.csv` into a Spark's DataFrame or Spark's RDD**, and process the data from there. You may assume the other two files (`keyfood_nyc_stores.json` and `keyfood_sample_items.csv`) are stored locally with your driver code.

### OUTPUT
Your output must be stored in another Spark's DataFrame or Spark's RDD, and being named as `outputTask1` at the end of the task. The column order must be the same as above (item name, price, and then % food insecurity). If your output is an RDD, each record should be a tuple of 3 elements. If your output is a Spark's DataFrame, it must have exactly 3 columns (you name them anything but they must in the same order).

### [TODO] A. Complete your code

Using either Spark's RDD or Spark's DataFrame transformations. The output must be placed in the `outputTask1` variable:

* Items must be filtered by UPC codes and names provided in the `keyfood_sample_items.csv`. UPC codes are considered equal if their numeric parts (the second portion after the `-`) are the same. For example, `SID-20308200000` is the same as `102-20308200000`, `KEY-20308200000`, etc.

* `Item Name` must be taken the sample items when there's a UPC code match (as defined above).

* `Price` should be extracted from the `price` column of `keyfood_products.csv`. The prefix `$` should be removed, and the output price should be converted to a float number (i.e. not a string).

*  `% Food Insecurity` is simply the percentage of the `foodInsecurity` score, i.e. by multiplying `foodInsecruity` by `100`.
"""

# Read keyfood_sample_items.csv into a Pandas DataFrame
df = pd.read_csv("keyfood_sample_items.csv")

# Convert Pandas DataFrame to Spark RDD
sample_rdd = spark.sparkContext.parallelize(df.values.tolist()) \
                  .map(lambda row: (row[0].split("-")[1], row[1]))

# Display the first 5 rows of the
print(sample_rdd.count())
sample_rdd.take(5)

keyfood = "keyfood_products.csv"
keyfood_rdd = sc.textFile(keyfood, use_unicode=True).cache()
list(enumerate(keyfood_rdd.first().split(',')))

def extractFeatures(partId, rows):
    if partId == 0:
        next(rows) # avoiding header here
    reader = csv.reader(rows)
    for fields in reader:
        store, upc, price = fields[0], fields[2], fields[5]
        if upc != "N/A":
            upc = upc.split("-")[1]
        else:
            continue
        price_parts = price.split("\xa0")
        price = price_parts[0].replace("$", "")
        yield (upc, (store, price))

keyfood_rdd = keyfood_rdd.mapPartitionsWithIndex(extractFeatures)
print(keyfood_rdd.count())
keyfood_rdd.take(5)

#Joining two rdd
dfA = sample_rdd.join(keyfood_rdd) \
                .map(lambda x: (x[1][1][0], (x[0], (x[1][1][1], x[1][0]))))

print(dfA.count())
dfA.take(5)

# load JSON file into a pandas DataFrame
df = pd.read_json('keyfood_nyc_stores.json')
df2 = df.transpose()
df3= df2.reset_index(level=0, inplace=False)
stores_df= df3 [['name','communityDistrict','foodInsecurity']]

# Create a Spark DataFrame from the Pandas DataFrame
stores_rdd = spark.createDataFrame(stores_df).rdd

# Transform the RDD to the desired format
stores_rdd2 = stores_rdd.map(lambda x: (x[0], (x[1], x[2]*100)))

# Print the output
stores_rdd2.take(5)

dfC = dfA.join(stores_rdd2)
dfC.take(5)

dfF = dfC.map(lambda x: (x[1][0][1][1], x[1][0][1][0], x[1][1][1]))
dfF.take(5)

outputTask1 = dfF.map(lambda x: (x[0], pd.to_numeric(x[1]), int(x[2])))
outputTask1.take(5)

## DO NOT EDIT BELOW
outputTask1 = outputTask1.cache()
outputTask1.count()

"""### B. Run to validate your output"""

#@title
def dfTask1(data):
    rdd = data.rdd if hasattr(data, 'rdd') else data
    if rdd.count()>10000:
        raise Exception('`outputTask1` has too many rows')
    rows = map(lambda x: (x[0], x[1], int(x[2])), rdd.collect())
    return pd.DataFrame(data=rows, columns=['Item Name','Price ($)','% Food Insecurity'])

def plotTask1(data, figsize=(8,8)):
    itemNames = pd.read_csv('keyfood_sample_items.csv')['Item Name']
    itemKey = dict(map(reversed,enumerate(itemNames)))
    df = dfTask1(data).sort_values(
        by = ['Item Name', '% Food Insecurity'],
        key = lambda x: list(map(lambda y: itemKey.get(y,y), x)))
    plt.figure(figsize=figsize)
    ax = sns.violinplot(x="Price ($)", y="Item Name", data=df, linewidth=0,
                        color='#ddd', scale='width', width=0.95)
    idx = len(ax.collections)
    sns.scatterplot(x="Price ($)", y="Item Name", hue='% Food Insecurity', data=df,
                    s=24, linewidth=0.5, edgecolor='gray', palette='YlOrRd')
    for h in ax.legend_.legendHandles:
        h.set_edgecolor('gray')
    pts = ax.collections[idx]
    pts.set_offsets(pts.get_offsets() + np.c_[np.zeros(len(df)),
                                            np.random.uniform(-.1, .1, len(df))])
    ax.set_xlim(left=0)
    ax.xaxis.grid(color='#eee')
    ax.yaxis.grid(color='#999')
    ax.set_title('Item Prices across KeyFood Stores in NYC')
    return ax

if 'outputTask1' not in locals():
    raise Exception('There is no `outputTask1` produced in Task 1.A')

plotTask1(outputTask1);

"""## Task 2 - Finding the Highest Priced Items in Areas with Food Insecurity

Examining the plot from Task 1, we could notice many cases where product prices are higher in areas with high food insecurity. For example, the highest priced *Cream O Land - Gallon 2% Milk* is in the area with a high `% Food Insecurity` value (the right most marker has a saturated red, approximately 20%). This suggests that our hypothesis might hold. At this point, we could perform a full *Null Hypothesis Test*, but before that, we would like to expand our study beyond just the sample items.

In particular, we would like to find all products that meets all of the conditions below:

1.  Must be sold in at least 3 stores, each with a food insecurity risk of `low`, `medium`, and `high`, respectively. The risk is based on the `foodInsecurity` value of each store, and computed as follows:

|foodInsecurity|Risk Rating|
|--|--|
|<=0.09|low|
|>0.09 and <=0.13| n/a|
|>0.13 and <=0.16| medium|
|>0.16 and <=0.23| n/a|
|>0.23| high|

2. The highest priced location has the risk rating of `high`.

3. The standard deviation of the product prices must be more than `$1`, i.e. when we collect all listed prices of the product based on its UPC, and compute the standard deviation, its value should be larger than `1`.

Your task is to find the list of all UPC codes (only the second part after the `-` in `SID-XXXXXXXXXXX`) that meet such conditions along with its `department` value.

### INPUT
**You must read `keyfood_products.csv` into a Spark's DataFrame or Spark's RDD**, and process the data from there. You may assume `keyfood_nyc_stores.json` is stored locally with your driver code.

### OUTPUT
Your output must be stored in another Spark's DataFrame or Spark's RDD, and being named as `outputTask2` at the end of the task.

### [TODO] A. Complete your code

Using either Spark's RDD or Spark's DataFrame transformations. The output must be placed in the `outputTask2` variable with the following column order:

| Extracted UPC Code | Item Name | Department |
|--|--|--|
|073296027686 | Urban Meadow - 100 Whole Wheat Bread | refrigerated |
|20308200000 | Broccoli Crowns | produce |
| ... | ... | ... |

* The data must be sorted by the **Extracted UPC Code** alphabetically (i.e. as strings and not as numbers).

* **Item Name** can be taken from any of the product instance.

*NOTE*: the output below is based on using Python's unbiased standard deviation (`statistics.stdev`). If you're using `statistics.pstdev`, you would see an answer of 94.
"""

keyfood = "keyfood_products.csv"
keyfood_rdd = sc.textFile(keyfood, use_unicode=True).cache()
def extractFeatures2(partId, rows):
    if partId == 0:
        next(rows) # avoiding header here
    reader = csv.reader(rows)
    for fields in reader:
        store, department, upc, price,name = fields[0], fields[1], fields[2], fields[5],fields[3]
        if upc != "N/A":
            upc = upc.split("-")[1]
        else:
            continue
        price_parts = price.split("\xa0")
        price = price_parts[0].replace("$", "")
        yield (upc, store,name, department, float(price))

keyfood_rdd2 = keyfood_rdd.mapPartitionsWithIndex(extractFeatures2).cache()
print(keyfood_rdd2.count())
keyfood_rdd2.take(5)

#Convert keyfood_rdd2 to a Spark DataFrame
keyfood_df = keyfood_rdd2.toDF(["UPC", "Store","ItemName", "Department", "Price"])
print(keyfood_df.count())
keyfood_df.show(5)

# load JSON file into a pandas DataFrame
df = pd.read_json('keyfood_nyc_stores.json')
df2 = df.transpose()
df3= df2.reset_index(level=0, inplace=False)
stores_df= df3 [['name','foodInsecurity']]

# Create a Spark DataFrame from the Pandas DataFrame
stores_df1 = spark.createDataFrame(stores_df) \
                   .withColumnRenamed("name", "Store")
stores_df1.show(5)

# Join the two dataframes on the Store column
joined_df1 = keyfood_df.join(stores_df1, "Store")

# Show the first five rows of the joined dataframe
joined_df1.count()

from pyspark.sql.functions import when, col, collect_list

# Join the dataframes and compute the risk column using when function
joined_df1 = keyfood_df.join(stores_df1, "Store") \
                     .withColumn("Risk",
                                 when(col("foodInsecurity") <= 0.09, "low") \
                                 .when((col("foodInsecurity") > 0.09) & (col("foodInsecurity") <= 0.13), "n/a") \
                                 .when((col("foodInsecurity") > 0.13) & (col("foodInsecurity") <= 0.16), "medium") \
                                 .when((col("foodInsecurity") > 0.16) & (col("foodInsecurity") <= 0.23), "n/a") \
                                 .when(col("foodInsecurity") > 0.23, "high") \
                                 .otherwise("unknown"))
print(joined_df1.count())
joined_df1.show(5)

from pyspark.sql.functions import collect_set, array_contains, size

# Group the joined dataframe by UPC and collect all Risk Rating values into a set
grouped_df = joined_df1.groupBy("UPC").agg(collect_set("Risk").alias("Risk"))

# Filter for the groups that contain all three ratings and count the number of ratings
counts_df = grouped_df.filter((array_contains("Risk", "low")) &
                              (array_contains("Risk", "medium")) &
                              (array_contains("Risk", "high"))) \
                      .withColumn("count", size("Risk")) \
                      .select("UPC", "count")
print(counts_df.count())
counts_df.show(5)

# Join with joined_df1 to get the full information about each product
final_df = joined_df1.join(counts_df, "UPC")

# Show the first ten rows of the final dataframe
print(final_df.count())
final_df.show(10)

from pyspark.sql.functions import max, stddev_samp

# Group final_df by UPC and Risk Rating, and find the max price for each group
max_prices_df = final_df.groupBy("UPC", "Risk").agg(max("Price").alias("Max Price"))
print(max_prices_df.count())
max_prices_df.show(5)

from pyspark.sql.functions import max, stddev_samp
from pyspark.sql.window import Window
from pyspark.sql.functions import col

# Group final_df by UPC and Risk Rating, and find the max price for each group
window = Window.partitionBy("UPC")
max_prices_df = final_df.groupBy("UPC").agg(max("Price").alias("Max Price"))

# Filter out the rows where the high risk store does not have the highest price
filtered_df = final_df.join(max_prices_df, "UPC") \
                      .filter((col("Risk") == "high") & (col("Price") == col("Max Price"))) \
                      .select(col("UPC"),col("Store"),col("Department"),col("Price"),col("foodInsecurity"),col("Risk"),col("ItemName"))

# Show the first ten rows of the filtered dataframe
print(filtered_df.count())
filtered_df.show(10)

final_df2 = final_df.alias('f1') \
                    .join(filtered_df.alias('f2'),col("f1.UPC") == col("f2.UPC"), 'inner') \
                    .select(col("f1.UPC"),col("f1.Store"),col("f1.Department"),col("f1.Price"),col("f1.Risk"),col("f1.ItemName"))

# Compute the standard deviation of the product prices for each UPC
stddev_df = final_df2.groupBy("UPC").agg(stddev_samp("Price").alias("stddev")) \
                                   .filter(col("stddev") > 1)
print(stddev_df.count())
stddev_df.show(5)

# Join the dataframes together
result_df = final_df2.join(stddev_df, "UPC")
print(result_df.count())
result_df.show(5)

outputTask2 = result_df.dropDuplicates(subset=['UPC'])

outputTask2 = outputTask2.select("UPC", "ItemName", "Department").sort("UPC")
outputTask2.show(5)

## DO NOT EDIT BELOW
outputTask2 = outputTask2.cache()
outputTask2.count()

"""### B. Run to validate your output"""

#@title
def dfTask2(data):
    rdd = data.rdd if hasattr(data, 'rdd') else data
    if rdd.count()>1000:
        raise Exception('`outputTask2` has too many rows')
    return pd.DataFrame(data=rdd.collect(),
                        columns=['Extracted UPC Code','ItemName','Department'])

if 'outputTask2' not in locals():
    raise Exception('There is no `outputTask2` produced in Task 2')

dfTask2(outputTask2).groupby('Department').size()

"""## Task 3 - Run on DataProc

You are asked to convert your Task 1 into a single `.py` file named `BDM_HW4_EMPLID_LastName.py` that can be executed on any DataProc cluster. Your file will be run with the following command:
"""

!gcloud dataproc jobs submit pyspark --cluster bdma --files keyfood_nyc_stores.json,keyfood_sample_items.csv \
--properties=spark.hadoop.fs.gs.requester.pays.mode=AUTO,spark.hadoop.fs.gs.requester.pays.project.id=YOUR_PROJECT_ID \
BDM_HW4_EMPLID_LastName.py -- OUTPUT_FOLDER_NAME

"""As part of the test, you must be able to run your code and output to the class shared folder, i.e.: `gs://bdma/shared/2023_spring/HW4/EMPLID_LastName`, **replacing `EMPLID` and `LastName` with yours**.

Note that, if you run your code multiple times, make sure to only run your working version when output to the shared folder, or you must remove the existing output to run your code again.

Notes:
* `YOUR_PROJECT_ID`: should be your project ID
* `OUTPUT_FOLDER_NAME`: being specified by the user at run-time. You can access this variable through `sys.argv[1]` in your code. Your code must output data into this folder (e.g. through `saveAsTextFile`).
* You may assume that both `keyfood_nyc_stores.json` and `keyfood_sample_items.csv` are in the current folder of the evaluation environment (or in yours). For testing purposes you may download the files into your code folder using the `gdown` command at the top of the notebook.
* `keyfood_products.csv` is expected to be on HDFS at `gs://bdma/data/keyfood_products.csv `, you can use this path directly with Spark.

Alternatively, to make it easier, you could write your configurations to a file and pass to the command line. For example, running the following two cells (still need to replace the script name, `YOUR_PROJECT_ID` and `OUTPUT_FOLDER_NAME` with appropriate values) would be equivalent to the command above:
"""

!pip install google-cloud-dataproc

!gcloud auth login

!gcloud projects list

!gcloud config set project big-data-380022
!gcloud config set compute/region us-west1
!gcloud config set compute/zone us-west1-a
!gcloud config set dataproc/region us-west1

# Commented out IPython magic to ensure Python compatibility.
# %%writefile props.conf
# spark.hadoop.fs.gs.requester.pays.mode=AUTO
# spark.hadoop.fs.gs.requester.pays.project.id=big-data-380022

!gcloud dataproc clusters create bdma --enable-component-gateway --region us-west1 --zone us-west1-a --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version 2.0-debian10 --project big-data-380022

!gcloud dataproc clusters list

!gcloud dataproc clusters describe bdma

# Commented out IPython magic to ensure Python compatibility.
# %%writefile BDM_HW4_24373710_Uddin.py
# #!/usr/bin/python
# 
# import csv
# import json
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# import pandas as pd
# import IPython
# IPython.display.set_matplotlib_formats('svg')
# pd.plotting.register_matplotlib_converters()
# sns.set_style("whitegrid")
# import sys
# 
# import pyspark
# from pyspark.sql import SparkSession
# from pyspark.sql import functions as F
# from pyspark.sql import types as T
# sc = pyspark.SparkContext.getOrCreate()
# spark = SparkSession(sc)
# spark
# 
# 
# # Read keyfood_sample_items.csv into a Pandas DataFrame
# df = pd.read_csv("keyfood_sample_items.csv")
# 
# # Convert Pandas DataFrame to Spark RDD
# sample_rdd = spark.sparkContext.parallelize(df.values.tolist()) \
#                   .map(lambda row: (row[0].split("-")[1], row[1]))
# 
# keyfood = 'gs://bdma/data/keyfood_products.csv'
# keyfood_rdd = sc.textFile(keyfood, use_unicode=True).cache()
# list(enumerate(keyfood_rdd.first().split(',')))
# def extractFeatures(partId, rows):
#     if partId == 0:
#         next(rows) # avoiding header here
#     reader = csv.reader(rows)
#     for fields in reader:
#         store, upc, price = fields[0], fields[2], fields[5]
#         if upc != "N/A":
#             upc = upc.split("-")[1]
#         else:
#             continue
#         price_parts = price.split("\xa0")
#         price = price_parts[0].replace("$", "")
#         yield (upc, (store, price))
# 
# keyfood_rdd = keyfood_rdd.mapPartitionsWithIndex(extractFeatures)
# dfA = sample_rdd.join(keyfood_rdd) \
#                 .map(lambda x: (x[1][1][0], (x[0], (x[1][1][1], x[1][0]))))
# 
# # load JSON file into a pandas DataFrame
# df = pd.read_json('keyfood_nyc_stores.json')
# df2 = df.transpose()
# df3= df2.reset_index(level=0, inplace=False)
# stores_df= df3 [['name','communityDistrict','foodInsecurity']]
# # Create a Spark DataFrame from the Pandas DataFrame
# stores_rdd = spark.createDataFrame(stores_df).rdd
# 
# # Transform the RDD to the desired format
# stores_rdd2 = stores_rdd.map(lambda x: (x[0], (x[1], x[2]*100)))
# dfC = dfA.join(stores_rdd2)
# dfF = dfC.map(lambda x: (x[1][0][1][1], x[1][0][1][0], x[1][1][1]))
# outputTask1 = dfF.map(lambda x: (x[0], pd.to_numeric(x[1]), int(x[2]))).cache()
# print(outputTask1.count())
# 
# filepath = sys.argv[1]
# 
# outputTask1.saveAsTextFile(filepath)

!gcloud dataproc jobs submit pyspark --cluster bdma --files keyfood_nyc_stores.json,keyfood_sample_items.csv \
--properties-file=props.conf BDM_HW4_24373710_Uddin.py -- gs://bdma/shared/2023_spring/HW4/24373710_Uddin

!gsutil -u big-data-380022 ls gs://bdma/shared/2023_spring/HW4

# !gsutil -u big-data-380022 cat gs://bdma/shared/2023_spring/HW4/24373710_Uddin/part*

# !gsutil -u big-data-380022 rm -r gs://bdma/shared/2023_spring/HW4/EMPLID_LastName/

"""# IMPORTANT: DELETE YOUR CLUSTER AFTER DONE"""

!gcloud dataproc clusters delete bdma -q
!gcloud dataproc clusters list

